{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2020 Xilinx Inc.\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolov3 with Vitis AI Tools\n",
    "This lab demonstrates the Vitis AI workflow on TensorFlow for a Yolov3 network trained on the Pascal VOC data set. Ths includes network freeze, quantization, evaluation, compilation and deployment on the ZCU104 Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow --user\n",
    "!pip install tqdm --user --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolov3 VOC Model \n",
    "\n",
    "The YoloV3 model we are using model was trained on the Pascal VOC data set, and is availabe on the Xilinx AI Model Zoo.\n",
    "The input image size is 416x416. The model has already been frozen.\n",
    "Visit https://github.com/Xilinx/Vitis-AI/tree/master/AI-Model-Zoo for more details.\n",
    "\n",
    "![title](pictures/vitis_ai_model_zoo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download TenfsorFlow Yolov3 VOC model from ModelZoo\n",
    "If you are running the Lab on the AWS AMI you can skip this step. The model has already been download for you.\n",
    "If you are running on your own machine uncomment the lines below to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O tf_yolov3_voc_416_416_1.0.zip  https://www.xilinx.com/bin/public/openDownload?filename=tf_yolov3_voc_416_416_1.0.zip \n",
    "#!unzip -j -d model_data tf_yolov3_voc_416_416_1.0.zip tf_yolov3_voc_416_416_65.63G/float/yolov3_voc.pb\n",
    "#!unzip -j -d model_data tf_yolov3_voc_416_416_1.0.zip tf_yolov3_voc_416_416_65.63G/test_code/voc_data/*.txt\n",
    "#!rm tf_yolov3_voc_416_416_1.0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DownLoad 2007 Pascal VOC Test Images\n",
    "We will be using the first 100 jpeg images in the 2007 Pascal VOC Data Set.\n",
    "If you are running the Lab on the AWS AMI you can skip this step. The files have already been downloaded for you.\n",
    "If you are running on your own machine uncomment the lines below to run\n",
    "\n",
    "#### Pascal 2007 VOC Citation:\n",
    "@misc{pascal-voc-2007,\n",
    "\tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n",
    "\ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n",
    "\thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "#!tar -xvf VOCtest_06-Nov-2007.tar\n",
    "#!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "#!tar -xvf VOCtrainval_06-Nov-2007.tar\n",
    "#!mkdir VOC2007\n",
    "#!mkdir VOC2007/JPEGImages\n",
    "#!cp VOCdevkit/VOC2007/JPEGImages/*.jpg VOC2007/JPEGImages/\n",
    "#!rm -rf VOCdevkit\n",
    "#!rm -f *.tar\n",
    "#!ln -s VOC2007/JPEGImages images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAI TensorFlow Quantization Tool (DECENT)\n",
    "### Inspect Networkt\n",
    "Use the \"inspect\" function in to check input nodes and output nodes of the network. We will use this information in later steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vai_q_tensorflow inspect --input_frozen_graph=model_data/yolov3_voc.pb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions with Floating Point Network\n",
    "The following python file will run inferrence by reading the images listed in the file: voc_data/2007_test.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python ./core/tf_prediction.py --pb_file model_data/yolov3_voc.pb      \\\n",
    "                                --test_list \"model_data/2007_test.txt\"    \\\n",
    "                                --result_file \"model_data/2007_test_pred.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The file voc_data/2007_test_pred.txt will be created which lists the detected class, score and box locations for all objects.\n",
    "\n",
    "\n",
    "![title](pictures/test_results.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute Floating Point  Network\n",
    "This python function will calculate Mean Average Precision (mAP) accuracy for the network using the images from the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./core/evaluation.py -mode detection \\\n",
    "                             -detection_use_07_metric True \\\n",
    "                             -gt_file ./model_data/2007_test_gt.txt \\\n",
    "                             -result_file ./model_data/2007_test_pred.txt \\\n",
    "                             -detection_iou 0.5 \\\n",
    "                             -detection_thresh 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluation is complete, you will see that the mAP for the images in the test data set is 0.7858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the Network\n",
    "The VAI quantizer (decnt) will now convert the floating point weights and activations to INT8. \n",
    "A small subset of the training data (typicall 100 images) is needed for quantization calibration. These are input into the the quantizer using the python based input function: input_fn.py (show below):\n",
    "\n",
    "![title](pictures/input_fn.PNG)\n",
    "\n",
    "The input function reads the calibration image and performs letterboxing (416x416 resize with aspect ratio preservation).\n",
    "\n",
    "This will take a few minutes to run on a GPU enabled machine, and much longer on a CPU only machine.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vai_q_tensorflow quantize \\\n",
    "  --input_frozen_graph model_data/yolov3_voc.pb \\\n",
    "  --input_nodes input_1 \\\n",
    "  --input_shapes ?,416,416,3 \\\n",
    "  --output_nodes conv2d_59/BiasAdd,conv2d_67/BiasAdd,conv2d_75/BiasAdd \\\n",
    "  --input_fn input_fn.calib_input \\\n",
    "  --method 1 \\\n",
    "  --gpu 0 \\\n",
    "  --calib_iter 100 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions with Quantized  Network\n",
    "Similar to what we did with the floating point model, we will now run inference and generate predictions using the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./core/tf_prediction.py --pb_file quantize_results/quantize_eval_model.pb      \\\n",
    "                                --test_list \"model_data/2007_test.txt\"    \\\n",
    "                                --result_file \"model_data/2007_test_quantize.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute the Quantized Network\n",
    "Now we will evaluate the Quantized network, and measure accuarcy after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ./core/evaluation.py -mode detection \\\n",
    "                             -detection_use_07_metric True \\\n",
    "                             -gt_file ./model_data/2007_test_gt.txt \\\n",
    "                             -result_file ./model_data/2007_test_quantize.txt \\\n",
    "                             -detection_iou 0.5 \\\n",
    "                             -detection_thresh 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mAP after quantization is 0.7744 as you can see there only a small accuracy loss converting from Float32 to INT8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAI Compilation Tool \n",
    "### Compile Network\n",
    "We will now compile the quantizied network. This will generate the instruction set for the Xilinx Deep Learning Processor Unit (DPU). The DPU is a configurable computation engine dedicated for convolutional neural networks. The --arch file point to a .json file which specifes the DPU configuration. In our case we are targetting the DPU config for the ZCU104 board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "\n",
      "Kernel topology \"yolov3_voc_kernel_graph.jpg\" for network \"yolov3_voc\"\n",
      "kernel list info for network \"yolov3_voc\"\n",
      "                               Kernel ID : Name\n",
      "                                       0 : yolov3_voc\n",
      "\n",
      "                             Kernel Name : yolov3_voc\n",
      "--------------------------------------------------------------------------------\n",
      "                             Kernel Type : DPUKernel\n",
      "                               Code Size : 0.93MB\n",
      "                              Param Size : 58.75MB\n",
      "                           Workload MACs : 65427.98MOPS\n",
      "                         IO Memory Space : 8.43MB\n",
      "                              Mean Value : 0, 0, 0, \n",
      "                      Total Tensor Count : 80\n",
      "                Boundary Input Tensor(s)   (H*W*C)\n",
      "                            input_1:0(0) : 416*416*3\n",
      "\n",
      "               Boundary Output Tensor(s)   (H*W*C)\n",
      "              conv2d_59_convolution:0(0) : 13*13*75\n",
      "              conv2d_67_convolution:0(1) : 26*26*75\n",
      "              conv2d_75_convolution:0(2) : 52*52*75\n",
      "\n",
      "                        Total Node Count : 79\n",
      "                           Input Node(s)   (H*W*C)\n",
      "                 conv2d_1_convolution(0) : 416*416*3\n",
      "\n",
      "                          Output Node(s)   (H*W*C)\n",
      "                conv2d_59_convolution(0) : 13*13*75\n",
      "                conv2d_67_convolution(0) : 26*26*75\n",
      "                conv2d_75_convolution(0) : 52*52*75\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "total 122320\n",
      "-rw-r--r-- 1 jim vitis-ai-users 62617944 Oct  1 15:51 dpu_yolov3_voc.elf\n",
      "-rw-r--r-- 1 jim vitis-ai-users 62619584 Sep 30 11:19 dpu_yolov3_voc_custom.elf\n",
      "-rw-r--r-- 1 jim vitis-ai-users     1421 Sep 30 11:19 yolov3_voc_custom_kernel.info\n",
      "-rw-r--r-- 1 jim vitis-ai-users      179 Sep 30 11:19 yolov3_voc_custom_kernel_graph.gv\n",
      "-rw-r--r-- 1 jim vitis-ai-users     1386 Oct  1 15:51 yolov3_voc_kernel.info\n",
      "-rw-r--r-- 1 jim vitis-ai-users      179 Oct  1 15:51 yolov3_voc_kernel_graph.gv\n"
     ]
    }
   ],
   "source": [
    "!vai_c_tensorflow --frozen_pb quantize_results/deploy_model.pb --arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/ZCU104/arch.json --output_dir vai_c_output --net_name yolov3_voc --options \"{'cpu_arch':'arm64', 'mode':'normal', 'save_kernel':''}\"\n",
    "!ls -l vai_c_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Instructions\n",
    "After compilation, the file dpu_yolov3_voc.elf is generated. This part of the lab completed, please go back to the terminal and enter CtrC to close the Jupyter Notebook server and then CtrlD to close the docker shell. You can also close this and any Jupyter notebooks you currently have open in your browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
